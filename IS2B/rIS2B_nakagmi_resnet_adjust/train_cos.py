import os
import torch
import torch.nn as nn
from torch import optim
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
import math


from model.resnet_pro import DilatedTimeResNet1D

from IS2B_x_pre import IS2B
from dataset.dataset import get_train_QPSKdataloader
from test_fig_x_pre import add_awgn_noise_torch

def train_IS2B_resnet_warmup(model, is2b, train_loader, val_loader, 
                             epochs=50, lr=5e-4, device='cuda',  # <--- å»ºè®®åˆå§‹ LR è®¾å¤§ä¸€ç‚¹
                             save_dir='./results_resnet_warmup', patience=15, 
                             sps=16, 
                             l1_weight=0.7, # ä¿æŒä¹‹å‰çš„é«˜ L1 æƒé‡å»ºè®®
                             warmup_epochs=5): # <--- æ–°å¢ï¼šçƒ­èº«è½®æ•°
    
    os.makedirs(save_dir, exist_ok=True)
    
    # === å®šä¹‰æ··åˆæŸå¤±å‡½æ•° ===
    criterion_l2 = nn.MSELoss() 
    criterion_l1 = nn.L1Loss()
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    
    # ==========================================
    # æ ¸å¿ƒä¿®æ”¹ï¼šWarmup + Cosine è°ƒåº¦å™¨ç»„åˆ
    # ==========================================
    # 1. çƒ­èº«è°ƒåº¦å™¨ï¼šåœ¨å‰ warmup_epochs è½®ï¼ŒLR ä» lr*0.1 çº¿æ€§å¢åŠ åˆ° lr
    scheduler_warmup = optim.lr_scheduler.LinearLR(
        optimizer, start_factor=0.1, end_factor=1.0, total_iters=warmup_epochs
    )
    
    # 2. ä¸»è°ƒåº¦å™¨ï¼šä½™å¼¦é€€ç«ï¼Œä» lr é™åˆ° eta_min
    scheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=epochs - warmup_epochs, eta_min=1e-6
    )
    
    # 3. ä¸²è”è°ƒåº¦å™¨ï¼šå…ˆè·‘ warmupï¼Œè·‘å®Œåè‡ªåŠ¨åˆ‡æ¢åˆ° cosine
    # milestones=[warmup_epochs] è¡¨ç¤ºåœ¨ç¬¬ 5 ä¸ª epoch åˆ‡æ¢
    scheduler = optim.lr_scheduler.SequentialLR(
        optimizer, schedulers=[scheduler_warmup, scheduler_cosine], milestones=[warmup_epochs]
    )
    # ==========================================
    
    loss_history = []
    val_loss_history = []
    best_val_loss = float('inf')
    epochs_since_improvement = 0

    print(f"ğŸš€ Start Training with Warmup({warmup_epochs}) + Cosine...")
    print(f"Base LR: {lr:.2e}, Loss: {l1_weight}*L1 + {1-l1_weight}*L2")

    for epoch in range(1, epochs + 1):
        model.train()
        epoch_loss = 0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{epochs}", leave=False, mininterval=2.0)
        
        for clean_x, faded_y, h_est in pbar:
            clean_x = clean_x.to(device).float()
            faded_y = faded_y.to(device).float()
            h_est = h_est.to(device).float()
            
            batch_size = clean_x.shape[0]
            seq_len = clean_x.shape[2]

            if h_est.dim() == 2:
                h_expanded = h_est.unsqueeze(-1).repeat(1, 1, seq_len)
            else:
                h_expanded = h_est

            # === ä¼˜åŒ–å»ºè®®ï¼šåŠ æƒ SNR é‡‡æ · (é‡ä½è½»é«˜) ===
            # è®© 70% çš„æ ·æœ¬è½åœ¨ 0-10dB (éš¾æ ·æœ¬)ï¼Œ30% è½åœ¨ 10-20dB
            r = torch.rand(batch_size, 1, 1, device=device)
            random_symbol_snr = torch.where(r < 0.7, r * (10/0.7), 10 + (r-0.7) * (10/0.3))
            random_sample_snr = random_symbol_snr - 10 * math.log10(sps)
            
            noisy_y = add_awgn_noise_torch(faded_y, random_sample_snr)

            # ç”Ÿæˆéšæœºæ—¶é—´æ­¥ t
            t_float = torch.rand(batch_size, device=device).view(-1, 1, 1)
            t_idx = (t_float.view(-1) * (is2b.n_steps - 1)).long()

            # æ„é€  I2SB ä¸­é—´æ€
            x_t = (1 - t_float) * clean_x + t_float * noisy_y

            # æ„é€ ç½‘ç»œè¾“å…¥
            net_input = torch.cat([x_t, h_expanded], dim=1)

            # é¢„æµ‹ä¸åå‘ä¼ æ’­
            optimizer.zero_grad()
            predicted_x0 = model(net_input, t_idx)
            
            # æ··åˆ Loss
            loss = l1_weight * criterion_l1(predicted_x0, clean_x) + (1 - l1_weight) * criterion_l2(predicted_x0, clean_x)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            epoch_loss += loss.item()
            pbar.set_postfix({'Loss': f"{loss.item():.5f}"})

        # === å…³é”®ï¼šæ›´æ–°å­¦ä¹ ç‡ (ä¸å†ä¼ å…¥ val_loss) ===
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        avg_train_loss = epoch_loss / len(train_loader)
        loss_history.append(avg_train_loss)

        # =======================
        #      éªŒè¯é˜¶æ®µ
        # =======================
        model.eval()
        val_loss = 0
        
        with torch.no_grad():
            for clean_x, faded_y, h_est in val_loader:
                clean_x = clean_x.to(device).float()
                faded_y = faded_y.to(device).float()
                h_est = h_est.to(device).float()
                
                batch_size = clean_x.shape[0]
                seq_len = clean_x.shape[2]
                if h_est.dim() == 2: h_expanded = h_est.unsqueeze(-1).repeat(1, 1, seq_len)
                else: h_expanded = h_est

                # éªŒè¯ SNR å›ºå®šä¸º 10dB
                val_sample_snr = 10.0 - 10 * math.log10(sps)
                noisy_y = add_awgn_noise_torch(faded_y, val_sample_snr)

                t_float = torch.rand(batch_size, device=device).view(-1, 1, 1)
                t_idx = (t_float.view(-1) * (is2b.n_steps - 1)).long()
                
                x_t = (1 - t_float) * clean_x + t_float * noisy_y
                net_input = torch.cat([x_t, h_expanded], dim=1)
                
                predicted_x0 = model(net_input, t_idx)
                
                # éªŒè¯ Loss
                loss_v = l1_weight * criterion_l1(predicted_x0, clean_x) + (1 - l1_weight) * criterion_l2(predicted_x0, clean_x)
                val_loss += loss_v.item()

        avg_val_loss = val_loss / len(val_loader)
        val_loss_history.append(avg_val_loss)
        
        print(f"[Epoch {epoch}] Train: {avg_train_loss:.5f} | Val: {avg_val_loss:.5f} | LR: {current_lr:.2e}")

        # ä¿å­˜ç­–ç•¥ (åªå­˜æœ€ä½³)
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            epochs_since_improvement = 0
            torch.save(model.state_dict(), os.path.join(save_dir, f"best_model_IS2B_resnet_cos.pth"))
            print("--> Best Model Saved.")
        else:
            epochs_since_improvement += 1

        # æ³¨æ„ï¼šCosine è°ƒåº¦é€šå¸¸å»ºè®®è·‘å®Œå…¨ç¨‹ï¼Œæ—©åœå¯ä»¥è®¾å¾—å®½å®¹ä¸€ç‚¹
        if epochs_since_improvement >= patience:
            print(f"Early stopping triggered.")
            break
            
    # ç”»å›¾
    plt.figure()
    plt.plot(loss_history, label='Train Loss')
    plt.plot(val_loss_history, label='Val Loss')
    plt.yscale('log')
    plt.title(f'Training Loss (Warmup+Cosine, L1 weight={l1_weight})')
    plt.legend()
    plt.savefig(os.path.join(save_dir, 'loss_curve.png'))
    print("Training Finished.")

if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    n_steps = 20 
    batch_size = 64
    epochs = 200 # Cosine ç­–ç•¥é€šå¸¸éœ€è¦è¾ƒé•¿çš„ epochs æ¥å……åˆ†ä¸‹é™
    sps = 16 
    
    # ç¡®ä¿å¯¼å…¥çš„æ˜¯å‡çº§ç‰ˆçš„ Pro ç½‘ç»œ
    from model.resnet_pro import DilatedTimeResNet1D
    
    print(f"Building DilatedTimeResNet1D on {device}...")
    model = DilatedTimeResNet1D(
        in_channels=4, 
        out_channels=2, 
        hidden_dim=256,   # æŒ‰ç…§å»ºè®®åŠ å®½ç½‘ç»œ
        num_blocks=8,     # ç¨å¾®å‡å°‘æ·±åº¦ï¼Œæ¢å–å®½åº¦
        time_emb_dim=128
    ).to(device)
    
    is2b_wrapper = IS2B(model, n_steps=n_steps, device=device)

    train_loader, val_loader = get_train_QPSKdataloader(
        start=0, end=400000, batch_size=batch_size, val_split=0.1
    )

    train_IS2B_resnet_warmup(
        model, is2b_wrapper,
        train_loader, val_loader,
        epochs=epochs,
        device=device,
        save_dir='IS2B/rIS2B_nakagmi_resnet_adjust/results', 
        sps=sps,
        patience=20, # Cosine å¯ä»¥åœ¨æœ€åé˜¶æ®µæ‰å¤§å¹…ä¸‹é™ï¼Œè€å¿ƒè¦ç»™è¶³
        l1_weight=0.8 # æé«˜ L1 æƒé‡ï¼Œå¢å¼ºé”åº¦
    )